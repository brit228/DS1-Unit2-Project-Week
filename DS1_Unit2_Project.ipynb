{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time for some Random Forests!\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/592/1*i0o8mjFfCn-uD79-F1Cqkw.png \"Random Forest Diagram\")\n",
    "\n",
    "So for this challenge, I decided to look into the scikit-learn Random Forest Classifier, which basically creates multiple decision trees and outputs the mode of the decision trees as the predicted class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, import some functions from sklearn, pandas, numpy and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder, OneHotEncoder, PolynomialFeatures, KBinsDiscretizer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And quickly create a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5408249158249159\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"train_features.csv\", index_col=0, header=0)\n",
    "y = pd.read_csv(\"train_labels.csv\", index_col=0, header=0)[\"status_group\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\n",
    "\n",
    "print((y_test == y_train.mode().values[0]).sum() / y_test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's look at Logistic Regression as a second baseline, while limiting the maximum number of categories for a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5408249158249159\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"train_features.csv\", index_col=0, header=0)\n",
    "y = pd.read_csv(\"train_labels.csv\", index_col=0, header=0)[\"status_group\"]\n",
    "\n",
    "X[\"date_recorded\"] = pd.to_numeric(pd.to_datetime(X[\"date_recorded\"]))\n",
    "\n",
    "X[\"region_code\"] = X[\"region_code\"].astype(str)\n",
    "X[\"district_code\"] = X[\"district_code\"].astype(str)\n",
    "\n",
    "X[[c for c in X if X[c].dtype == 'object']] = X[[c for c in X if X[c].dtype == 'object']].astype(str).fillna(\"NAN\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\n",
    "\n",
    "cols = [c for c in X_train if X_train[c].dtype == 'object' and len(X_train[c].astype('category').cat.categories) < 1000]\n",
    "dropcols = [c for c in X_train if X_train[c].dtype == 'object']\n",
    "        \n",
    "o = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(X_train[cols])\n",
    "X_train = pd.concat([X_train.drop(columns=dropcols, axis=1), pd.DataFrame(o.transform(X_train[cols]), columns=o.get_feature_names(cols), index=X_train.index)], axis=1)\n",
    "X_test = pd.concat([X_test.drop(columns=dropcols, axis=1), pd.DataFrame(o.transform(X_test[cols]), columns=o.get_feature_names(cols), index=X_test.index)], axis=1)\n",
    "\n",
    "l = LogisticRegression().fit(X_train, y_train)\n",
    "print(l.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much better...\n",
    "\n",
    "However, there are some weird values for some of the numerical features, so let's solve those first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5408249158249159\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"train_features.csv\", index_col=0, header=0)\n",
    "y = pd.read_csv(\"train_labels.csv\", index_col=0, header=0)[\"status_group\"]\n",
    "\n",
    "X[\"date_recorded\"] = pd.to_numeric(pd.to_datetime(X[\"date_recorded\"]))\n",
    "\n",
    "X[\"region_code\"] = X[\"region_code\"].astype(str)\n",
    "X[\"district_code\"] = X[\"district_code\"].astype(str)\n",
    "\n",
    "X[[c for c in X if X[c].dtype == 'object']] = X[[c for c in X if X[c].dtype == 'object']].astype(str).fillna(\"NAN\")\n",
    "\n",
    "X['longitude'] = X['longitude'].replace(0.0, np.nan)\n",
    "X['longitude_bool'] = (X[\"longitude\"] == np.nan).astype(int)\n",
    "\n",
    "X['latitude'] = X['latitude'].replace(-2.000000e-08, np.nan)\n",
    "X['latitude_bool'] = (X[\"longitude\"] == np.nan).astype(int)\n",
    "\n",
    "X['num_private'] = X['num_private'].replace(0.0, np.nan)\n",
    "X[\"num_private_bool\"] = (X[\"num_private\"] == np.nan).astype(int)\n",
    "\n",
    "X['construction_year'] = X['construction_year'].replace(0.0, np.nan)\n",
    "X[\"construction_year_bool\"] = (X[\"construction_year\"] == np.nan).astype(int)\n",
    "\n",
    "X['population'] = X['population'].replace(0.0, np.nan)\n",
    "X[\"population_bool\"] = (X[\"population\"] == np.nan).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\n",
    "\n",
    "longitude_gm = X_train.groupby(['region', 'district_code'])['longitude'].transform('mean')\n",
    "X_train['longitude'] = X_train['longitude'].fillna(longitude_gm)\n",
    "longitude_m = X_train['longitude'].mean()\n",
    "X_train['longitude'] = X_train['longitude'].fillna(longitude_m)\n",
    "\n",
    "X_test['longitude'] = X_test['longitude'].fillna(longitude_gm)\n",
    "X_test['longitude'] = X_test['longitude'].fillna(longitude_m)\n",
    "\n",
    "\n",
    "latitude_gm = X_train.groupby(['region', 'district_code'])['latitude'].transform('mean')\n",
    "X_train['latitude'] = X_train['latitude'].fillna(latitude_gm)\n",
    "latitude_m = X_train['latitude'].mean()\n",
    "X_train['latitude'] = X_train['latitude'].fillna(latitude_m)\n",
    "\n",
    "X_test['latitude'] = X_test['latitude'].fillna(latitude_gm)\n",
    "X_test['latitude'] = X_test['latitude'].fillna(latitude_m)\n",
    "\n",
    "num_private_m = X_train['num_private'].mean()\n",
    "X_train['num_private'] = X_train['num_private'].fillna(num_private_m)\n",
    "X_test['num_private'] = X_test['num_private'].fillna(num_private_m)\n",
    "\n",
    "construction_year_m = X_train['construction_year'].mean()\n",
    "X_train['construction_year'] = X_train['construction_year'].fillna(construction_year_m)\n",
    "X_test['construction_year'] = X_test['construction_year'].fillna(construction_year_m)\n",
    "\n",
    "population_m = X_train['population'].mean()\n",
    "X_train['population'] = X_train['population'].fillna(population_m)\n",
    "X_test['population'] = X_test['population'].fillna(population_m)\n",
    "\n",
    "dropcols = [c for c in X_train if X_train[c].dtype == 'object']\n",
    "        \n",
    "o = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(X_train[cols])\n",
    "X_train = pd.concat([X_train.drop(columns=dropcols, axis=1), pd.DataFrame(o.transform(X_train[cols]), columns=o.get_feature_names(cols), index=X_train.index)], axis=1)\n",
    "X_test = pd.concat([X_test.drop(columns=dropcols, axis=1), pd.DataFrame(o.transform(X_test[cols]), columns=o.get_feature_names(cols), index=X_test.index)], axis=1)\n",
    "\n",
    "l = LogisticRegression().fit(X_train, y_train)\n",
    "print(l.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still nothing huh...\n",
    "\n",
    "Let's add some stuff! (Inverses, bins, and power transforms of numerical features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7552188552188552\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"train_features.csv\", index_col=0, header=0)\n",
    "y = pd.read_csv(\"train_labels.csv\", index_col=0, header=0)[\"status_group\"]\n",
    "\n",
    "X[\"date_recorded\"] = pd.to_numeric(pd.to_datetime(X[\"date_recorded\"]))\n",
    "\n",
    "X[\"region_code\"] = X[\"region_code\"].astype(str)\n",
    "X[\"district_code\"] = X[\"district_code\"].astype(str)\n",
    "\n",
    "X[[c for c in X if X[c].dtype == 'object']] = X[[c for c in X if X[c].dtype == 'object']].astype(str).fillna(\"NAN\")\n",
    "\n",
    "X['longitude'] = X['longitude'].replace(0.0, np.nan)\n",
    "X['longitude_bool'] = (X[\"longitude\"] == np.nan).astype(int)\n",
    "\n",
    "X['latitude'] = X['latitude'].replace(-2.000000e-08, np.nan)\n",
    "X['latitude_bool'] = (X[\"longitude\"] == np.nan).astype(int)\n",
    "\n",
    "X['num_private'] = X['num_private'].replace(0.0, np.nan)\n",
    "X[\"num_private_bool\"] = (X[\"num_private\"] == np.nan).astype(int)\n",
    "\n",
    "X['construction_year'] = X['construction_year'].replace(0.0, np.nan)\n",
    "X[\"construction_year_bool\"] = (X[\"construction_year\"] == np.nan).astype(int)\n",
    "\n",
    "X['population'] = X['population'].replace(0.0, np.nan)\n",
    "X[\"population_bool\"] = (X[\"population\"] == np.nan).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\n",
    "\n",
    "numcols = [c for c in X_train if X_train[c].dtype != 'object']\n",
    "invcols = [c for c in X_train if X_train[c].dtype != 'object' and (X_train[c] == 0).sum() == 0]\n",
    "cols = [c for c in X_train if X_train[c].dtype == 'object' and len(X_train[c].astype('category').cat.categories) < 1000]\n",
    "\n",
    "longitude_gm = X_train.groupby(['region', 'district_code'])['longitude'].transform('mean')\n",
    "X_train['longitude'] = X_train['longitude'].fillna(longitude_gm)\n",
    "longitude_m = X_train['longitude'].mean()\n",
    "X_train['longitude'] = X_train['longitude'].fillna(longitude_m)\n",
    "\n",
    "X_test['longitude'] = X_test['longitude'].fillna(longitude_gm)\n",
    "X_test['longitude'] = X_test['longitude'].fillna(longitude_m)\n",
    "\n",
    "\n",
    "latitude_gm = X_train.groupby(['region', 'district_code'])['latitude'].transform('mean')\n",
    "X_train['latitude'] = X_train['latitude'].fillna(latitude_gm)\n",
    "latitude_m = X_train['latitude'].mean()\n",
    "X_train['latitude'] = X_train['latitude'].fillna(latitude_m)\n",
    "\n",
    "X_test['latitude'] = X_test['latitude'].fillna(latitude_gm)\n",
    "X_test['latitude'] = X_test['latitude'].fillna(latitude_m)\n",
    "\n",
    "num_private_m = X_train['num_private'].mean()\n",
    "X_train['num_private'] = X_train['num_private'].fillna(num_private_m)\n",
    "X_test['num_private'] = X_test['num_private'].fillna(num_private_m)\n",
    "\n",
    "construction_year_m = X_train['construction_year'].mean()\n",
    "X_train['construction_year'] = X_train['construction_year'].fillna(construction_year_m)\n",
    "X_test['construction_year'] = X_test['construction_year'].fillna(construction_year_m)\n",
    "\n",
    "population_m = X_train['population'].mean()\n",
    "X_train['population'] = X_train['population'].fillna(population_m)\n",
    "X_test['population'] = X_test['population'].fillna(population_m)\n",
    "\n",
    "dropcols = [c for c in X_train if X_train[c].dtype == 'object']\n",
    "        \n",
    "o = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(X_train[cols])\n",
    "X_train = pd.concat([X_train.drop(columns=dropcols, axis=1), pd.DataFrame(o.transform(X_train[cols]), columns=o.get_feature_names(cols), index=X_train.index)], axis=1)\n",
    "X_test = pd.concat([X_test.drop(columns=dropcols, axis=1), pd.DataFrame(o.transform(X_test[cols]), columns=o.get_feature_names(cols), index=X_test.index)], axis=1)\n",
    "\n",
    "for c in invcols:\n",
    "    X_train[\"{}(inv)\".format(c)] = 1.0 / X_train[c]\n",
    "    X_test[\"{}(inv)\".format(c)] = 1.0 / X_test[c]\n",
    "    \n",
    "k = KBinsDiscretizer(n_bins=100, encode=\"onehot-dense\", strategy=\"uniform\").fit(X_train[numcols])\n",
    "kcols = [\"{}[{}]\".format(c,i) for ind,c in enumerate(numcols) for i in range(k.n_bins_[ind])]\n",
    "X_train = pd.concat([X_train, pd.DataFrame(k.transform(X_train[numcols]), columns=kcols, index=X_train.index)], axis=1)\n",
    "X_test = pd.concat([X_test, pd.DataFrame(k.transform(X_test[numcols]), columns=kcols, index=X_test.index)], axis=1)\n",
    "\n",
    "m = PowerTransformer().fit(X_train[numcols])\n",
    "X_train[numcols] = m.transform(X_train[numcols])\n",
    "X_test[numcols] = m.transform(X_test[numcols])\n",
    "\n",
    "l = LogisticRegression().fit(X_train, y_train)\n",
    "print(l.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some progress! Now let's add some interaction. However, there will be way too many terms, so let's limit it with a select percentile (ANOVA) to a max of 200 pre-polynomial and a max of 1000 post-polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7361952861952862\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"train_features.csv\", index_col=0, header=0)\n",
    "y = pd.read_csv(\"train_labels.csv\", index_col=0, header=0)[\"status_group\"]\n",
    "\n",
    "X[\"date_recorded\"] = pd.to_numeric(pd.to_datetime(X[\"date_recorded\"]))\n",
    "\n",
    "X[\"region_code\"] = X[\"region_code\"].astype(str)\n",
    "X[\"district_code\"] = X[\"district_code\"].astype(str)\n",
    "\n",
    "X[[c for c in X if X[c].dtype == 'object']] = X[[c for c in X if X[c].dtype == 'object']].astype(str).fillna(\"NAN\")\n",
    "\n",
    "X['longitude'] = X['longitude'].replace(0.0, np.nan)\n",
    "X['longitude_bool'] = (X[\"longitude\"] == np.nan).astype(int)\n",
    "\n",
    "X['latitude'] = X['latitude'].replace(-2.000000e-08, np.nan)\n",
    "X['latitude_bool'] = (X[\"longitude\"] == np.nan).astype(int)\n",
    "\n",
    "X['num_private'] = X['num_private'].replace(0.0, np.nan)\n",
    "X[\"num_private_bool\"] = (X[\"num_private\"] == np.nan).astype(int)\n",
    "\n",
    "X['construction_year'] = X['construction_year'].replace(0.0, np.nan)\n",
    "X[\"construction_year_bool\"] = (X[\"construction_year\"] == np.nan).astype(int)\n",
    "\n",
    "X['population'] = X['population'].replace(0.0, np.nan)\n",
    "X[\"population_bool\"] = (X[\"population\"] == np.nan).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\n",
    "\n",
    "numcols = [c for c in X_train if X_train[c].dtype != 'object']\n",
    "invcols = [c for c in X_train if X_train[c].dtype != 'object' and (X_train[c] == 0).sum() == 0]\n",
    "cols = [c for c in X_train if X_train[c].dtype == 'object' and len(X_train[c].astype('category').cat.categories) < 1000]\n",
    "\n",
    "longitude_gm = X_train.groupby(['region', 'district_code'])['longitude'].transform('mean')\n",
    "X_train['longitude'] = X_train['longitude'].fillna(longitude_gm)\n",
    "longitude_m = X_train['longitude'].mean()\n",
    "X_train['longitude'] = X_train['longitude'].fillna(longitude_m)\n",
    "\n",
    "X_test['longitude'] = X_test['longitude'].fillna(longitude_gm)\n",
    "X_test['longitude'] = X_test['longitude'].fillna(longitude_m)\n",
    "\n",
    "latitude_gm = X_train.groupby(['region', 'district_code'])['latitude'].transform('mean')\n",
    "X_train['latitude'] = X_train['latitude'].fillna(latitude_gm)\n",
    "latitude_m = X_train['latitude'].mean()\n",
    "X_train['latitude'] = X_train['latitude'].fillna(latitude_m)\n",
    "\n",
    "X_test['latitude'] = X_test['latitude'].fillna(latitude_gm)\n",
    "X_test['latitude'] = X_test['latitude'].fillna(latitude_m)\n",
    "\n",
    "num_private_m = X_train['num_private'].mean()\n",
    "X_train['num_private'] = X_train['num_private'].fillna(num_private_m)\n",
    "X_test['num_private'] = X_test['num_private'].fillna(num_private_m)\n",
    "\n",
    "construction_year_m = X_train['construction_year'].mean()\n",
    "X_train['construction_year'] = X_train['construction_year'].fillna(construction_year_m)\n",
    "X_test['construction_year'] = X_test['construction_year'].fillna(construction_year_m)\n",
    "\n",
    "population_m = X_train['population'].mean()\n",
    "X_train['population'] = X_train['population'].fillna(population_m)\n",
    "X_test['population'] = X_test['population'].fillna(population_m)\n",
    "\n",
    "dropcols = [c for c in X_train if X_train[c].dtype == 'object']\n",
    "        \n",
    "o = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(X_train[cols])\n",
    "X_train = pd.concat([X_train.drop(columns=dropcols, axis=1), pd.DataFrame(o.transform(X_train[cols]), columns=o.get_feature_names(cols), index=X_train.index)], axis=1)\n",
    "X_test = pd.concat([X_test.drop(columns=dropcols, axis=1), pd.DataFrame(o.transform(X_test[cols]), columns=o.get_feature_names(cols), index=X_test.index)], axis=1)\n",
    "\n",
    "for c in invcols:\n",
    "    X_train[\"{}(inv)\".format(c)] = 1.0 / X_train[c]\n",
    "    X_test[\"{}(inv)\".format(c)] = 1.0 / X_test[c]\n",
    "    \n",
    "k = KBinsDiscretizer(n_bins=100, encode=\"onehot-dense\", strategy=\"uniform\").fit(X_train[numcols])\n",
    "kcols = [\"{}[{}]\".format(c,i) for ind,c in enumerate(numcols) for i in range(k.n_bins_[ind])]\n",
    "X_train = pd.concat([X_train, pd.DataFrame(k.transform(X_train[numcols]), columns=kcols, index=X_train.index)], axis=1)\n",
    "X_test = pd.concat([X_test, pd.DataFrame(k.transform(X_test[numcols]), columns=kcols, index=X_test.index)], axis=1)\n",
    "\n",
    "m = PowerTransformer().fit(X_train[numcols])\n",
    "X_train[numcols] = m.transform(X_train[numcols])\n",
    "X_test[numcols] = m.transform(X_test[numcols])\n",
    "\n",
    "p_val1 = SelectPercentile(percentile=(200*100)//X_train.shape[1]).fit(X_train, y_train)\n",
    "X_train = p_val1.transform(X_train)\n",
    "X_test = p_val1.transform(X_test)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2).fit(X_train)\n",
    "X_train = poly.transform(X_train)\n",
    "X_test = poly.transform(X_test)\n",
    "\n",
    "p_val2 = SelectPercentile(percentile=(1000*100)//X_train.shape[1]).fit(X_train, y_train)\n",
    "X_train = p_val2.transform(X_train)\n",
    "X_test = p_val2.transform(X_test)\n",
    "\n",
    "l = LogisticRegression().fit(X_train, y_train)\n",
    "print(l.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so not so great. Let's look at Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   31.2s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   57.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    1.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7675925925925926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    1.6s finished\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"train_features.csv\", index_col=0, header=0)\n",
    "y = pd.read_csv(\"train_labels.csv\", index_col=0, header=0)[\"status_group\"]\n",
    "\n",
    "X[\"date_recorded\"] = pd.to_numeric(pd.to_datetime(X[\"date_recorded\"]))\n",
    "\n",
    "X[\"region_code\"] = X[\"region_code\"].astype(str)\n",
    "X[\"district_code\"] = X[\"district_code\"].astype(str)\n",
    "\n",
    "X[[c for c in X if X[c].dtype == 'object']] = X[[c for c in X if X[c].dtype == 'object']].astype(str).fillna(\"NAN\")\n",
    "\n",
    "X['longitude'] = X['longitude'].replace(0.0, np.nan)\n",
    "X['longitude_bool'] = (X[\"longitude\"] == np.nan).astype(int)\n",
    "\n",
    "X['latitude'] = X['latitude'].replace(-2.000000e-08, np.nan)\n",
    "X['latitude_bool'] = (X[\"longitude\"] == np.nan).astype(int)\n",
    "\n",
    "X['num_private'] = X['num_private'].replace(0.0, np.nan)\n",
    "X[\"num_private_bool\"] = (X[\"num_private\"] == np.nan).astype(int)\n",
    "\n",
    "X['construction_year'] = X['construction_year'].replace(0.0, np.nan)\n",
    "X[\"construction_year_bool\"] = (X[\"construction_year\"] == np.nan).astype(int)\n",
    "\n",
    "X['population'] = X['population'].replace(0.0, np.nan)\n",
    "X[\"population_bool\"] = (X[\"population\"] == np.nan).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\n",
    "\n",
    "ordin = OrdinalEncoder()\n",
    "ordin.fit(y_train.values.reshape(-1,1))\n",
    "y_tr_c = ordin.transform(y_train.values.reshape(-1,1)).reshape(-1)\n",
    "y_te_c = ordin.transform(y_test.values.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "numcols = [c for c in X_train if X_train[c].dtype != 'object']\n",
    "invcols = [c for c in X_train if X_train[c].dtype != 'object' and (X_train[c] == 0).sum() == 0]\n",
    "cols = [c for c in X_train if X_train[c].dtype == 'object' and len(X_train[c].astype('category').cat.categories) < 1000]\n",
    "\n",
    "longitude_gm = X_train.groupby(['region', 'district_code'])['longitude'].transform('mean')\n",
    "X_train['longitude'] = X_train['longitude'].fillna(longitude_gm)\n",
    "longitude_m = X_train['longitude'].mean()\n",
    "X_train['longitude'] = X_train['longitude'].fillna(longitude_m)\n",
    "\n",
    "X_test['longitude'] = X_test['longitude'].fillna(longitude_gm)\n",
    "X_test['longitude'] = X_test['longitude'].fillna(longitude_m)\n",
    "\n",
    "\n",
    "latitude_gm = X_train.groupby(['region', 'district_code'])['latitude'].transform('mean')\n",
    "X_train['latitude'] = X_train['latitude'].fillna(latitude_gm)\n",
    "latitude_m = X_train['latitude'].mean()\n",
    "X_train['latitude'] = X_train['latitude'].fillna(latitude_m)\n",
    "\n",
    "X_test['latitude'] = X_test['latitude'].fillna(latitude_gm)\n",
    "X_test['latitude'] = X_test['latitude'].fillna(latitude_m)\n",
    "\n",
    "num_private_m = X_train['num_private'].mean()\n",
    "X_train['num_private'] = X_train['num_private'].fillna(num_private_m)\n",
    "X_test['num_private'] = X_test['num_private'].fillna(num_private_m)\n",
    "\n",
    "construction_year_m = X_train['construction_year'].mean()\n",
    "X_train['construction_year'] = X_train['construction_year'].fillna(construction_year_m)\n",
    "X_test['construction_year'] = X_test['construction_year'].fillna(construction_year_m)\n",
    "\n",
    "population_m = X_train['population'].mean()\n",
    "X_train['population'] = X_train['population'].fillna(population_m)\n",
    "X_test['population'] = X_test['population'].fillna(population_m)\n",
    "\n",
    "dropcols = [c for c in X_train if X_train[c].dtype == 'object']\n",
    "        \n",
    "o = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(X_train[cols])\n",
    "X_train = pd.concat([X_train.drop(columns=dropcols, axis=1), pd.DataFrame(o.transform(X_train[cols]), columns=o.get_feature_names(cols), index=X_train.index)], axis=1)\n",
    "X_test = pd.concat([X_test.drop(columns=dropcols, axis=1), pd.DataFrame(o.transform(X_test[cols]), columns=o.get_feature_names(cols), index=X_test.index)], axis=1)\n",
    "\n",
    "for c in invcols:\n",
    "    X_train[\"{}(inv)\".format(c)] = 1.0 / X_train[c]\n",
    "    X_test[\"{}(inv)\".format(c)] = 1.0 / X_test[c]\n",
    "    \n",
    "k = KBinsDiscretizer(n_bins=100, encode=\"onehot-dense\", strategy=\"uniform\").fit(X_train[numcols])\n",
    "kcols = [\"{}[{}]\".format(c,i) for ind,c in enumerate(numcols) for i in range(k.n_bins_[ind])]\n",
    "X_train = pd.concat([X_train, pd.DataFrame(k.transform(X_train[numcols]), columns=kcols, index=X_train.index)], axis=1)\n",
    "X_test = pd.concat([X_test, pd.DataFrame(k.transform(X_test[numcols]), columns=kcols, index=X_test.index)], axis=1)\n",
    "\n",
    "m = PowerTransformer().fit(X_train[numcols])\n",
    "X_train[numcols] = m.transform(X_train[numcols])\n",
    "X_test[numcols] = m.transform(X_test[numcols])\n",
    "\n",
    "p_val1 = SelectPercentile(percentile=(200*100)//X_train.shape[1]).fit(X_train, y_train)\n",
    "X_train = p_val1.transform(X_train)\n",
    "X_test = p_val1.transform(X_test)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2).fit(X_train)\n",
    "X_train = poly.transform(X_train)\n",
    "X_test = poly.transform(X_test)\n",
    "\n",
    "p_val2 = SelectPercentile(percentile=(1000*100)//X_train.shape[1]).fit(X_train, y_train)\n",
    "X_train = p_val2.transform(X_train)\n",
    "X_test = p_val2.transform(X_test)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, verbose=1, n_jobs=-1, random_state=42).fit(X_train, y_tr_c)\n",
    "print(clf.score(X_test, y_te_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Lets add some sigmoid calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   46.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   21.8s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   37.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   47.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   36.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   46.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    1.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7882154882154883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    1.8s finished\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"train_features.csv\", index_col=0, header=0)\n",
    "y = pd.read_csv(\"train_labels.csv\", index_col=0, header=0)[\"status_group\"]\n",
    "\n",
    "X[\"date_recorded\"] = pd.to_numeric(pd.to_datetime(X[\"date_recorded\"]))\n",
    "\n",
    "X[\"region_code\"] = X[\"region_code\"].astype(str)\n",
    "X[\"district_code\"] = X[\"district_code\"].astype(str)\n",
    "\n",
    "X[[c for c in X if X[c].dtype == 'object']] = X[[c for c in X if X[c].dtype == 'object']].astype(str).fillna(\"NAN\")\n",
    "\n",
    "X['longitude'] = X['longitude'].replace(0.0, np.nan)\n",
    "X['longitude_bool'] = (X[\"longitude\"] == np.nan).astype(int)\n",
    "\n",
    "X['latitude'] = X['latitude'].replace(-2.000000e-08, np.nan)\n",
    "X['latitude_bool'] = (X[\"longitude\"] == np.nan).astype(int)\n",
    "\n",
    "X['num_private'] = X['num_private'].replace(0.0, np.nan)\n",
    "X[\"num_private_bool\"] = (X[\"num_private\"] == np.nan).astype(int)\n",
    "\n",
    "X['construction_year'] = X['construction_year'].replace(0.0, np.nan)\n",
    "X[\"construction_year_bool\"] = (X[\"construction_year\"] == np.nan).astype(int)\n",
    "\n",
    "X['population'] = X['population'].replace(0.0, np.nan)\n",
    "X[\"population_bool\"] = (X[\"population\"] == np.nan).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\n",
    "\n",
    "ordin = OrdinalEncoder()\n",
    "ordin.fit(y_train.values.reshape(-1,1))\n",
    "y_tr_c = ordin.transform(y_train.values.reshape(-1,1)).reshape(-1)\n",
    "y_te_c = ordin.transform(y_test.values.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "numcols = [c for c in X_train if X_train[c].dtype != 'object']\n",
    "invcols = [c for c in X_train if X_train[c].dtype != 'object' and (X_train[c] == 0).sum() == 0]\n",
    "cols = [c for c in X_train if X_train[c].dtype == 'object' and len(X_train[c].astype('category').cat.categories) < 1000]\n",
    "\n",
    "longitude_gm = X_train.groupby(['region', 'district_code'])['longitude'].transform('mean')\n",
    "X_train['longitude'] = X_train['longitude'].fillna(longitude_gm)\n",
    "longitude_m = X_train['longitude'].mean()\n",
    "X_train['longitude'] = X_train['longitude'].fillna(longitude_m)\n",
    "\n",
    "X_test['longitude'] = X_test['longitude'].fillna(longitude_gm)\n",
    "X_test['longitude'] = X_test['longitude'].fillna(longitude_m)\n",
    "\n",
    "\n",
    "latitude_gm = X_train.groupby(['region', 'district_code'])['latitude'].transform('mean')\n",
    "X_train['latitude'] = X_train['latitude'].fillna(latitude_gm)\n",
    "latitude_m = X_train['latitude'].mean()\n",
    "X_train['latitude'] = X_train['latitude'].fillna(latitude_m)\n",
    "\n",
    "X_test['latitude'] = X_test['latitude'].fillna(latitude_gm)\n",
    "X_test['latitude'] = X_test['latitude'].fillna(latitude_m)\n",
    "\n",
    "num_private_m = X_train['num_private'].mean()\n",
    "X_train['num_private'] = X_train['num_private'].fillna(num_private_m)\n",
    "X_test['num_private'] = X_test['num_private'].fillna(num_private_m)\n",
    "\n",
    "construction_year_m = X_train['construction_year'].mean()\n",
    "X_train['construction_year'] = X_train['construction_year'].fillna(construction_year_m)\n",
    "X_test['construction_year'] = X_test['construction_year'].fillna(construction_year_m)\n",
    "\n",
    "population_m = X_train['population'].mean()\n",
    "X_train['population'] = X_train['population'].fillna(population_m)\n",
    "X_test['population'] = X_test['population'].fillna(population_m)\n",
    "\n",
    "dropcols = [c for c in X_train if X_train[c].dtype == 'object']\n",
    "        \n",
    "o = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(X_train[cols])\n",
    "X_train = pd.concat([X_train.drop(columns=dropcols, axis=1), pd.DataFrame(o.transform(X_train[cols]), columns=o.get_feature_names(cols), index=X_train.index)], axis=1)\n",
    "X_test = pd.concat([X_test.drop(columns=dropcols, axis=1), pd.DataFrame(o.transform(X_test[cols]), columns=o.get_feature_names(cols), index=X_test.index)], axis=1)\n",
    "\n",
    "for c in invcols:\n",
    "    X_train[\"{}(inv)\".format(c)] = 1.0 / X_train[c]\n",
    "    X_test[\"{}(inv)\".format(c)] = 1.0 / X_test[c]\n",
    "    \n",
    "k = KBinsDiscretizer(n_bins=100, encode=\"onehot-dense\", strategy=\"uniform\").fit(X_train[numcols])\n",
    "kcols = [\"{}[{}]\".format(c,i) for ind,c in enumerate(numcols) for i in range(k.n_bins_[ind])]\n",
    "X_train = pd.concat([X_train, pd.DataFrame(k.transform(X_train[numcols]), columns=kcols, index=X_train.index)], axis=1)\n",
    "X_test = pd.concat([X_test, pd.DataFrame(k.transform(X_test[numcols]), columns=kcols, index=X_test.index)], axis=1)\n",
    "\n",
    "m = PowerTransformer().fit(X_train[numcols])\n",
    "X_train[numcols] = m.transform(X_train[numcols])\n",
    "X_test[numcols] = m.transform(X_test[numcols])\n",
    "\n",
    "p_val1 = SelectPercentile(percentile=(200*100)//X_train.shape[1]).fit(X_train, y_train)\n",
    "X_train = p_val1.transform(X_train)\n",
    "X_test = p_val1.transform(X_test)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2).fit(X_train)\n",
    "X_train = poly.transform(X_train)\n",
    "X_test = poly.transform(X_test)\n",
    "\n",
    "p_val2 = SelectPercentile(percentile=(1000*100)//X_train.shape[1]).fit(X_train, y_train)\n",
    "X_train = p_val2.transform(X_train)\n",
    "X_test = p_val2.transform(X_test)\n",
    "\n",
    "clf = CalibratedClassifierCV(RandomForestClassifier(n_estimators=1000, verbose=1, n_jobs=-1, random_state=42), method=\"sigmoid\", cv=3).fit(X_train, y_tr_c)\n",
    "print(clf.score(X_test, y_te_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's leave it as that and predict the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    2.1s finished\n"
     ]
    }
   ],
   "source": [
    "X_t = pd.read_csv(\"test_features.csv\", index_col=0, header=0)\n",
    "indexes = X_t.index.values\n",
    "\n",
    "X_t[\"date_recorded\"] = pd.to_numeric(pd.to_datetime(X_t[\"date_recorded\"]))\n",
    "X_t[\"region_code\"] = X_t[\"region_code\"].astype(str)\n",
    "X_t[\"district_code\"] = X_t[\"district_code\"].astype(str)\n",
    "X_t[[c for c in X_t if X_t[c].dtype == 'object']] = X_t[[c for c in X_t if X_t[c].dtype == 'object']].astype(str).fillna(\"NAN\")\n",
    "X_t['longitude'] = X_t['longitude'].replace(0.0, np.nan)\n",
    "X_t['longitude_bool'] = (X_t[\"longitude\"] == np.nan).astype(int)\n",
    "X_t['latitude'] = X_t['latitude'].replace(-2.000000e-08, np.nan)\n",
    "X_t['latitude_bool'] = (X_t[\"longitude\"] == np.nan).astype(int)\n",
    "X_t['num_private'] = X_t['num_private'].replace(0.0, np.nan)\n",
    "X_t[\"num_private_bool\"] = (X_t[\"num_private\"] == np.nan).astype(int)\n",
    "X_t['construction_year'] = X_t['construction_year'].replace(0.0, np.nan)\n",
    "X_t[\"construction_year_bool\"] = (X_t[\"construction_year\"] == np.nan).astype(int)\n",
    "X_t['population'] = X_t['population'].replace(0.0, np.nan)\n",
    "X_t[\"population_bool\"] = (X_t[\"population\"] == np.nan).astype(int)\n",
    "X_t['longitude'] = X_t['longitude'].fillna(longitude_gm)\n",
    "X_t['longitude'] = X_t['longitude'].fillna(longitude_m)\n",
    "X_t['latitude'] = X_t['latitude'].fillna(latitude_gm)\n",
    "X_t['latitude'] = X_t['latitude'].fillna(latitude_m)\n",
    "X_t['num_private'] = X_t['num_private'].fillna(num_private_m)\n",
    "X_t['construction_year'] = X_t['construction_year'].fillna(construction_year_m)\n",
    "X_t['population'] = X_t['population'].fillna(population_m)\n",
    "X_t = pd.concat([X_t.drop(columns=dropcols, axis=1), pd.DataFrame(o.transform(X_t[cols]), columns=o.get_feature_names(cols), index=X_t.index)], axis=1)\n",
    "for c in invcols:\n",
    "    X_t[\"{}(inv)\".format(c)] = 1.0 / X_t[c]\n",
    "X_t = pd.concat([X_t, pd.DataFrame(k.transform(X_t[numcols]), columns=kcols, index=X_t.index)], axis=1)\n",
    "X_t[numcols] = m.transform(X_t[numcols])\n",
    "X_t = p_val1.transform(X_t)\n",
    "X_t = poly.transform(X_t)\n",
    "X_t = p_val2.transform(X_t)\n",
    "\n",
    "pd.DataFrame(np.concatenate([indexes.reshape(-1,1), ordin.inverse_transform(clf.predict(X_t).reshape(-1,1))], axis=1), columns=[\"id\", \"status_group\"]).to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
